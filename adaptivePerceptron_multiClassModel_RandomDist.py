import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


x_uniform = 3.5*np.random.random((100, 128))
x_normal = np.random.normal(size = (100,128))
x_poisson = np.random.poisson(lam = 1., size = (100, 128))

x_all = np.vstack((x_uniform, x_normal, x_poisson))

y_uniform = np.array(np.zeros(shape = (x_uniform.shape[0],1)) - 1)  # Label -1
y_normal = np.array(np.zeros(shape = (x_normal.shape[0],1)) + 1)       # Label +1
y_poisson = np.array(np.zeros(shape = (x_poisson.shape[0],1)))# Label 0

y_all = np.vstack((y_uniform, y_normal, y_poisson)).astype('int32')

########################################################################
#############################################
#####################
########

class adaptive_perceptron(object):
    def __init__(self, learning_rate = 0.01, num_itrs = 15):
        self.learning_rate = learning_rate
        self.num_itrs = num_itrs
        
    def fit_model(self, input_data, labels_):
        self.weights_ = np.zeros(shape = (input_data.shape[1]+1,1))
        self.cost_ = np.zeros(shape = (self.num_itrs,1))
        self.acc_ = np.copy(self.cost_)
        for i in range(self.num_itrs):
            # out_ = self.net_input(input_data)
            # out_ = np.array(out_).reshape((out_.shape[0],1))
            err_ = labels_ - self.predict(input_data)
            self.weights_[1:] += self.learning_rate*input_data.T.dot(err_)
            self.weights_[0] += self.learning_rate*err_.sum()
            self.cost_[i] = np.sum(err_**2)
            self.acc_[i] = self.accuracy_score(input_data, labels_)
        return self

    def net_input(self, input_data):
        return np.dot(input_data, self.weights_[1:]) + self.weights_[0]

    def predict(self, input_data):
        preds_ = []
        for val in np.nditer(self.net_input(input_data)):
            if val > self.net_input(input_data).mean() - self.net_input(input_data).mean()/2:
                preds_.append(1)
            elif val < self.net_input(input_data).mean()+self.net_input(input_data).mean()/2:
                preds_.append(-1)
            else: preds_.append(0)
        return np.array(preds_).reshape((len(preds_),1))
    
    def accuracy_score(self, _data, _labels):
        counts = 0
        for idx, est in zip(np.arange(0, _data.shape[0],1), self.predict(_data)):
            if est == _labels[idx]: counts += 1
 
        return counts/_data.shape[0]*100


p_multi = adaptive_perceptron(learning_rate = 0.1, num_itrs = 50)

p_multi.fit_model(x_all, y_all)
fit_vals = p_multi.net_input(x_all)
loss_ = np.array(p_multi.cost_).reshape((len(p_multi.cost_),1))

plt.figure()
plt.plot(np.arange(1, loss_.shape[0] + 1), loss_, label = 'Loss')
plt.plot(np.arange(1, p_multi.acc_.shape[0]+1), p_multi.acc_, label = 'Accuracy')
plt.xlabel('Epochs'), plt.ylabel('Performance')
plt.title('Perceptron performance: Classifying 3 random distribution types\n from each other')
plt.legend(), plt.show()

plt.figure()
plt.hist(fit_vals[:100], bins = 100, color = 'r', label = 'Uniform dist.', cumulative = True)
plt.hist(fit_vals[200:], bins = 100, color = 'b', label = 'Poisson dist.', cumulative = True)
plt.hist(fit_vals[100:200], bins = 100, color = 'g', label = 'Normal dist.', cumulative = True)
plt.xlabel('Weights generated'), plt.ylabel('Counts')
plt.title('Histogram of network weights generated by input distributions')
plt.legend()
plt.show()
########################################################################
#############################################
#####################
########

from matplotlib.colors import ListedColormap

markers = ('x', 'o','s')
colors = ('red','blue', 'green')

x1 = 4*np.random.random(size = (100, 128))
x2 = np.random.normal(size = (100, 128))
x3 = np.random.poisson(size = (100, 128))

unifit = p_multi.net_input(x1)
normfit = p_multi.net_input(x2)
poisfit = p_multi.net_input(x3)

# 
plt.figure()
plt.hist(unifit, bins = unifit.shape[0], color = colors[0], label = 'Uniform dist.')
plt.hist(poisfit, bins = poisfit.shape[0], color = colors[1], label = 'Poisson dist.')
plt.hist(normfit, bins = normfit.shape[0], color = colors[2], label = 'Normal dist.')
plt.xlabel('Weights generated'), plt.ylabel('Counts')
plt.title('Histogram of network weights generated by 3 test distributions')
plt.legend()
plt.show()

plt.figure()
plt.hist(unifit, bins = unifit.shape[0], cumulative = True, color = colors[0], label = 'Uniform dist.')
plt.hist(poisfit, bins = poisfit.shape[0], cumulative = True, color = colors[1], label = 'Poisson dist.')
plt.hist(normfit, bins = normfit.shape[0], cumulative = True, color = colors[2], label = 'Normal dist.')
plt.xlabel('Weights generated'), plt.ylabel('Number of sample sequences')
plt.title('Cumulative distribution of weights generated by \n 3 test distributions')
plt.legend()
plt.show()