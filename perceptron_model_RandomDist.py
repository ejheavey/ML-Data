import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

########################################################################
#############################################
#####################
########
x_uniform = 3.5*np.random.random((100, 128))
x_normal = np.random.normal(size = (100,128))
x_poisson = np.random.poisson(lam = 1., size = (100, 128))

x_all = np.vstack((x_uniform, x_normal, x_poisson))

y_uniform = np.array(np.zeros(shape = (x_uniform.shape[0],1)) + 1)
y_normal = np.array(np.zeros(shape = (x_normal.shape[0],1)))
y_poisson = np.array(np.zeros(shape = (x_poisson.shape[0],1)) - 1)

y_all = np.vstack((y_uniform, y_normal, y_poisson))
y = np.where(y_all == 0, 1, -1)

class perceptron(object):
    def __init__(self, learning_rate = 0.01, num_itrs = 15):
        self.learning_rate = learning_rate
        self.num_itrs = num_itrs
        
    def fit_model(self, input_data, labels_):
        self.weights_ = np.zeros(shape = (1, input_data.shape[1]+1))
        self.errors_ = []
        for _ in range(self.num_itrs):
            errs = 0
            for x, targ in zip(input_data, labels_):
                update = self.learning_rate*(targ - self.predict(x))
                self.weights_[0,1:] += update*x
                self.weights_[0,0] += update
                errs += int(update != 0.)
            self.errors_.append(errs)
        return self

    def net_input(self, input_data):
        return np.dot(input_data, self.weights_[0,1:]) + self.weights_[0,0]

    def predict(self, input_data):
        return np.where(self.net_input(input_data) > 0., 1, -1)

p_binary = perceptron(learning_rate = 0.01, num_itrs = 10)

p_binary.fit_model(x_all, y)
fit_vals = p_binary.net_input(x_all)
percent_errs = np.array(p_binary.errors_).reshape((len(p_binary.errors_),1))/fit_vals.shape[0]*100
acc_ = 100*np.ones(shape = (percent_errs.shape[0],1)) - percent_errs

plt.figure()
plt.plot(np.arange(1, percent_errs.shape[0] + 1), percent_errs, label = 'Error')
plt.plot(np.arange(1, acc_.shape[0]+1), acc_, label = 'Accuracy')
plt.xlabel('Epochs'), plt.ylabel('Performance, %')
plt.title('Perceptron performance: Classifying normal dists. against\n uniform and Poisson dists. (normal dist. vs. not normal dist.)')
plt.legend()

########################################################################
#############################################
#####################
########

from matplotlib.colors import ListedColormap

markers = ('x', 'o','s')
colors = ('red','blue', 'green')
# 
# cmap = ListedColormap(colors[:len(np.unique(y_all))])
# plt.figure()
# for i in range(len(x_all)):
#     if y_all[i] > 0:
#         plt.scatter(np.arange(0, len(x_all[i]), 1), x_all[i], c = cmap.colors[0], marker = markers[0])
#     elif y_all[i] < 0:
#         plt.scatter(np.arange(0, len(x_all[i]), 1), x_all[i], c = cmap.colors[1], marker = markers[1])
#     else: plt.scatter(np.arange(0, len(x_all[i]), 1), x_all[i], c = cmap.colors[2], marker = markers[2])

x1 = 4*np.random.random(size = (50, 128)) # Uniform
x2 = np.random.normal(size = (50, 128))  # Normal
x3 = np.random.poisson(size = (50, 128))# Poisson

unifit = p_binary.net_input(x1)
normfit = p_binary.net_input(x2)
poisfit = p_binary.net_input(x3)

plt.figure()
plt.hist(unifit, bins = unifit.shape[0], color = colors[0], label = 'Uniform dist.')
plt.hist(poisfit, bins = poisfit.shape[0], color = colors[1], label = 'Poisson dist.')
plt.hist(normfit, bins = normfit.shape[0], color = colors[2], label = 'Normal dist.')
plt.xlabel('Weights generated'), plt.ylabel('Counts')
plt.title('Histogram of network weights generated by 3 distribution types in\n binary classification where normal dists. = 1, not normal = -1')
plt.legend()
plt.show()


plt.figure()
plt.hist(unifit, bins = unifit.shape[0], cumulative = True, color = colors[0], label = 'Uniform dist.')
plt.hist(poisfit, bins = poisfit.shape[0], cumulative = True, color = colors[1], label = 'Poisson dist.')
plt.hist(normfit, bins = normfit.shape[0], cumulative = True, color = colors[2], label = 'Normal dist.')
plt.xlabel('Weights generated'), plt.ylabel('Number of sample sequences')
plt.title('Cumulative distribution of weights generated in binary classification for\n 3 distribution types where normal dist. = 1, not normal = -1')
plt.legend()
plt.show()

### MODEL CONVERGES QUICKLY IN ACCURACY WHEN FITTING (< 20 EPOCHS),
## TESTS ~90% ACCURATE ON NEW RANDOM DATA
